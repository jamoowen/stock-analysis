{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1c39d73-2676-41aa-b623-49ccccd3f140",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jowen/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/08 21:04:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as psf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import time\n",
    "import requests\n",
    "import pandas as pandas\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from helpers import request_prices_polygon, round_cols\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "# Access the API key\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "\n",
    "# Read in csv and replace FB and ANTM tickers (they have changed)\n",
    "df_csv = spark.read.csv(\"./input_data/stocks.csv\", header=True, sep=',') \\\n",
    "    .withColumn('symbol', psf.regexp_replace('symbol', 'FB', 'META')) \\\n",
    "    .withColumn('symbol', psf.regexp_replace('symbol', 'ANTM', 'ELV')) \\\n",
    "    .withColumn('initial_investment', psf.lit(10000))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2068c2d7-a1eb-4857-8121-095e9d8ba699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+------------------+------+----------+\n",
      "|        company_name|symbol|initial_investment| price|      date|\n",
      "+--------------------+------+------------------+------+----------+\n",
      "|          3M Company|   MMM|           10000.0| 93.84|2024-02-06|\n",
      "|          3M Company|   MMM|           10000.0|162.41|2022-02-08|\n",
      "|           AT&T Inc.|     T|           10000.0| 17.33|2024-02-06|\n",
      "|           AT&T Inc.|     T|           10000.0| 23.94|2022-02-08|\n",
      "|         AbbVie Inc.|  ABBV|           10000.0|175.01|2024-02-06|\n",
      "|         AbbVie Inc.|  ABBV|           10000.0|143.51|2022-02-08|\n",
      "| Abbott Laboratories|   ABT|           10000.0|113.31|2024-02-06|\n",
      "| Abbott Laboratories|   ABT|           10000.0|128.65|2022-02-08|\n",
      "|Accenture Plc Cla...|   ACN|           10000.0|366.65|2024-02-06|\n",
      "|Accenture Plc Cla...|   ACN|           10000.0|345.07|2022-02-08|\n",
      "|  Adobe Incorporated|  ADBE|           10000.0|615.85|2024-02-06|\n",
      "|  Adobe Incorporated|  ADBE|           10000.0|511.31|2022-02-08|\n",
      "|Advanced Micro De...|   AMD|           10000.0|170.94|2024-02-06|\n",
      "|Advanced Micro De...|   AMD|           10000.0|128.23|2022-02-08|\n",
      "|Alphabet Inc. Cla...| GOOGL|           10000.0|145.54|2024-02-06|\n",
      "|Alphabet Inc. Cla...| GOOGL|           10000.0| 139.4|2022-02-08|\n",
      "|Alphabet Inc. Cla...|  GOOG|           10000.0|146.68|2024-02-06|\n",
      "|Alphabet Inc. Cla...|  GOOG|           10000.0|139.21|2022-02-08|\n",
      "|    Altria Group Inc|    MO|           10000.0| 40.26|2024-02-06|\n",
      "|    Altria Group Inc|    MO|           10000.0| 50.42|2022-02-08|\n",
      "+--------------------+------+------------------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet('./outputs/polygon_stock_data.parquet')\n",
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\",\"dynamic\")\n",
    "df.orderBy('company_name').show()\n",
    "# test that pulling new data only works \n",
    "# df_mod = df.withColumn(\"date\", psf.expr(\"CASE when date=='2024-02-07' THEN to_date('2024-02-06') else date end\"))\n",
    "# df_mod.write.mode('overwrite').partitionBy('date').format('parquet').save('./outputs/polygon_stock_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ad22411d-5acf-4ba4-a59a-785ecd5fdcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_share_prices(df):\n",
    "    print(\"comparing the share prices across dates\")\n",
    "    max_date = df.select(psf.max(\"date\")).collect()[0][0]\n",
    "    min_date = df.select(psf.min(\"date\")).collect()[0][0]\n",
    "    max_window = window = Window.partitionBy(\"initial_investment\").orderBy(psf.col('change_percentage').desc())\n",
    "    \n",
    "    df_compare = df.filter(f\"date ='{max_date}' or date ='{min_date}'\") \\\n",
    "        .groupBy([\"company_name\", \"symbol\", \"initial_investment\"]).pivot(\"date\").sum('price') \\\n",
    "        .withColumn('no_shares', psf.col('initial_investment')/psf.col(f'{min_date}')) \\\n",
    "        .withColumn('current_value', psf.col('no_shares')*psf.col(f'{max_date}')) \\\n",
    "        .withColumn('change_percentage', (psf.try_subtract(f'{max_date}', f'{min_date}'))/psf.col(f'{min_date}')*100) \\\n",
    "        .withColumn(\"rank\", psf.rank().over(max_window)) \n",
    "    \n",
    "    df_compare = round_cols(df_compare, cols=[x for x in df_compare.columns if x not in ('company_name', 'symbol', 'date', 'rank')])\n",
    "    print(max_date, min_date)\n",
    "\n",
    "    print(f\"writing comparison outputs to: ./outputs/polygon_stock_comparison.csv\")\n",
    "    pdf = df_compare.toPandas()\n",
    "    pdf.to_csv('./outputs/polygon_stock_comparison.csv', sep=',', encoding='utf-8', index=False)\n",
    "\n",
    "    # sum of all the current investments' value\n",
    "    sum_of_investments = df_compare.groupBy().sum().collect()\n",
    "    greatest_relative_inrease = df_compare.filter(\"rank==1\").collect()[0]\n",
    "    initial_investment_total = sum_of_investments[0]['sum(initial_investment)']\n",
    "    current_investment_total = sum_of_investments[0]['sum(current_value)']\n",
    "    \n",
    "    # create path if doesnt exist\n",
    "    Path(\"./outputs\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    \n",
    "    # write answers to txt file\n",
    "    print(f\"writing comparison results to: ./outputs/results.txt\")\n",
    "    with open('./outputs/results.txt', 'w') as f:\n",
    "        print('Stock price comparison\\n', f'Current date: {current_date}\\n', f'Start date: {old_date}\\n' , file=f)\n",
    "    \n",
    "    with open('./outputs/results.txt', 'a') as f:\n",
    "      print(\"Greatest relative increase: \", greatest_relative_inrease['company_name'], f\"({greatest_relative_inrease['symbol']})\",file=f)\n",
    "      print(\"Growth %: \", greatest_relative_inrease['change_percentage'],file=f)\n",
    "      print(\"Gross profit %: \", greatest_relative_inrease['current_value']-greatest_relative_inrease['initial_investment'], '\\n',file=f)\n",
    "      \n",
    "      print(\"Initial total investment: \", initial_investment_total, file=f)\n",
    "      print(\"Current total investment (growth): \", current_investment_total, file=f)\n",
    "        \n",
    "    df_compare.show()\n",
    "# df_compare.groupBy(\"company_name\").pivot(\"date\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8752cd9c-999b-4a98-ae91-8c85dfd1ec67",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\",\"dynamic\")\n",
    "\n",
    "\n",
    "current_date = (datetime.today() - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "old_date = (datetime.today() - relativedelta(years=2)).strftime('%Y-%m-%d')\n",
    "\n",
    "select_expression = [\"company_name\", \"symbol\", \"cast(initial_investment as double) as initial_investment\", \"CAST(date as date) as date\", \"CAST(price as double) as price\"]\n",
    "\n",
    "if (Path.cwd() / 'outputs' / 'polygon_stock_data.csv').exists():\n",
    "    df_polygon_data = spark.read.csv(\"./outputs/polygon_stock_data.csv\", header=True, inferSchema=True, sep=',')\n",
    "    max_date = df_polygon_data.select(psf.max(\"date\")).collect()[0][0]\n",
    "    min_date = df_polygon_data.select(psf.min(\"date\")).collect()[0][0]\n",
    "\n",
    "    if datetime.strptime(current_date, '%Y-%m-%d').date() > max_date:\n",
    "        print(\"pulling new data available, requesting now...\")\n",
    "        dict_new = request_prices_polygon(stocks_dict=stocks_dict, date_period=current_date, api_key=api_key)\n",
    "        df_new = spark.createDataFrame(dict_new).selectExpr(select_expression)\n",
    "        df_new.write.mode('overwrite').format('parquet').save('./outputs/polygon_stock_data.parquet')\n",
    "    \n",
    "    if datetime.strptime(old_date, '%Y-%m-%d').date() < min_date:\n",
    "        print(\"Older data available, requesting now...\")\n",
    "        dict_old = request_prices_polygon(stocks_dict=stocks_dict, date_period=old_date, api_key=api_key)\n",
    "        df_old = spark.createDataFrame(dict_old).selectExpr(select_expression)\n",
    "        df_old.write.mode('overwrite').format('parquet').save('./outputs/polygon_stock_data.parquet')\n",
    "\n",
    "    pri\n",
    "    df = spark.read.parquet('./outputs/polygon_stock_data.parquet')\n",
    "    \n",
    "    pdf = df.toPandas()\n",
    "    pdf.to_csv('./outputs/polygon_stock_data.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24c1c51a-1bb0-4bd7-a4c0-d91618b979fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\",\"dynamic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42a4d894-5517-4412-90d2-06d44a4084ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[company_name: string, symbol: string, initial_investment: double, date: date, price: double]\n",
      "+--------------------+------+------------------+----------+------+\n",
      "|        company_name|symbol|initial_investment|      date| price|\n",
      "+--------------------+------+------------------+----------+------+\n",
      "|          Apple Inc.|  AAPL|           10000.0|2024-02-07|189.41|\n",
      "|Microsoft Corpora...|  MSFT|           10000.0|2024-02-07|414.05|\n",
      "|     Amazon.com Inc.|  AMZN|           10000.0|2024-02-07|170.53|\n",
      "|           Tesla Inc|  TSLA|           10000.0|2024-02-07|187.58|\n",
      "|Alphabet Inc. Cla...| GOOGL|           10000.0|2024-02-07|145.54|\n",
      "|Alphabet Inc. Cla...|  GOOG|           10000.0|2024-02-07|146.68|\n",
      "|Berkshire Hathawa...| BRK.B|           10000.0|2024-02-07|397.66|\n",
      "|   Johnson & Johnson|   JNJ|           10000.0|2024-02-07|157.98|\n",
      "|UnitedHealth Grou...|   UNH|           10000.0|2024-02-07|519.39|\n",
      "|  NVIDIA Corporation|  NVDA|           10000.0|2024-02-07|700.99|\n",
      "|Meta Platforms In...|  META|           10000.0|2024-02-07|469.59|\n",
      "|Procter & Gamble ...|    PG|           10000.0|2024-02-07|159.12|\n",
      "|JPMorgan Chase & Co.|   JPM|           10000.0|2024-02-07|175.43|\n",
      "|Exxon Mobil Corpo...|   XOM|           10000.0|2024-02-07|102.22|\n",
      "|   Visa Inc. Class A|     V|           10000.0|2024-02-07|279.39|\n",
      "|     Home Depot Inc.|    HD|           10000.0|2024-02-07|362.69|\n",
      "| Chevron Corporation|   CVX|           10000.0|2024-02-07|152.13|\n",
      "|Mastercard Incorp...|    MA|           10000.0|2024-02-07|461.91|\n",
      "|         AbbVie Inc.|  ABBV|           10000.0|2024-02-07|175.01|\n",
      "|         Pfizer Inc.|   PFE|           10000.0|2024-02-07| 27.56|\n",
      "+--------------------+------+------------------+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "select_expression = [\"company_name\", \"symbol\", \"cast(initial_investment as double) as initial_investment\", \"CAST(date as date) as date\", \"CAST(price as double) as price\"]\n",
    "df_polygon_data = spark.read.csv(\"./outputs/polygon_stock_data.csv\", header=True, sep=',').selectExpr(select_expression)\n",
    "print(df_polygon_data)\n",
    "df_polygon_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "662c87d6-712a-4e58-950a-5d9ba8b43526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+-----+\n",
      "|date|company_name|count|\n",
      "+----+------------+-----+\n",
      "+----+------------+-----+\n",
      "\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "df_test = spark.read.parquet('./outputs/polygon_stock_data.parquet')\n",
    "df_test.groupBy(['date', 'company_name']).count().filter('count>1').show()\n",
    "\n",
    "duplicates = df_test.groupBy(['date', 'company_name']).count().filter('count>1').count()\n",
    "print(duplicates==0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
